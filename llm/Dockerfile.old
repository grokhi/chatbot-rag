# Use NVIDIA CUDA base image with Ubuntu
FROM nvidia/cuda:12.1.1-base-ubuntu20.04

# Set non-interactive mode for apt
ENV DEBIAN_FRONTEND=noninteractive

# Define a build argument for the model name
ARG MODEL_NAME=llama3.1:8b
ENV MODEL_NAME=${MODEL_NAME}


# Install essential dependencies
RUN apt-get update && apt-get install -y \
    wget \
    curl \
    gnupg \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Install NVIDIA Container Toolkit inside the container
# RUN curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
#     && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \
#     | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \
#     | tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \
#     && apt-get update && apt-get install -y nvidia-container-toolkit

# Install Ollama
RUN curl -fsSL https://ollama.com/install.sh | bash


# Expose the default port for Ollama
EXPOSE 11434

# Pre-pull the specified model
# RUN ollama serve & \
#     until curl -s http://localhost:11434; do \
#     echo "Waiting for Ollama server..."; \
#     sleep 2; \
#     done && \
#     ollama pull ${MODEL_NAME} && \
#     pkill -f "ollama serve"


# Create a volume for Ollama data
VOLUME /root/.ollama

# Start Ollama server
CMD ["ollama", "serve"]
