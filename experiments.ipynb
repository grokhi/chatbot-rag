{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-70b-versatile\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grokhi/.cache/pypoetry/virtualenvs/chatbot-rag-0NodYq9M-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import getpass\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "file_path = \"data/datasets/RuBQ_2.0_dev.json\"\n",
    "documents = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "for item in data:\n",
    "    content = (\n",
    "        f\"Question (RU): {item['question_text']}\\n\"\n",
    "        f\"Question (EN): {item.get('question_eng', '')}\\n\"\n",
    "        f\"Answer: {item['answer_text']}\\n\"\n",
    "    )\n",
    "    metadata = {\n",
    "        \"uid\": item[\"uid\"],\n",
    "        \"tags\": item[\"tags\"],\n",
    "        \"version\": item[\"RuBQ_version\"],\n",
    "    }\n",
    "    documents.append(Document(page_content=content, metadata=metadata))\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "filter_complex_metadata(documents)\n",
    "splitted_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "_vectorstore = Chroma.from_documents(\n",
    "    documents=splitted_docs,\n",
    "    collection_name=\"qa_chroma\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "retriever = _vectorstore.as_retriever()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool(\"retrieve\", response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    # retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(description=\"Binary 'yes' or 'no' relevance score\")\n",
    "\n",
    "\n",
    "# Define the prompt\n",
    "system = (\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question.\"\n",
    "    \"If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\n",
    "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",
    "    # \"Dont forget to consider chat history when answering the question.\"\n",
    ")\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Document: {document} Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "retrieval_grader = grade_prompt | llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "\n",
    "@tool(\"grade\")\n",
    "def grade(query: str, retrieved_docs: List[Document]):\n",
    "    \"\"\"Grade relevance of retrieved documents.\"\"\"\n",
    "    lst = []\n",
    "    for doc in retrieved_docs:\n",
    "        result = retrieval_grader.invoke(\n",
    "            {\"question\": query, \"document\": doc.page_content}\n",
    "        )\n",
    "        lst.append(result.binary_score)\n",
    "\n",
    "    if all(lst) == 'no':\n",
    "        return 'web search'\n",
    "    return 'generate answer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "agent_executor = create_react_agent(llm, [retrieve, grade], checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Какой город является столицей Туркмении?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (call_pqf8)\n",
      " Call ID: call_pqf8\n",
      "  Args:\n",
      "    query: столица Туркмении\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'uid': 22, 'version': '1.0'}\n",
      "Content: Question (RU): Какой город является столицей Туркмении?\n",
      "Question (EN): Which city is the capital of Turkmenistan?\n",
      "Answer: Ашхабад\n",
      "\n",
      "Source: {'uid': 56, 'version': '1.0'}\n",
      "Content: Question (RU): Как называлась столица Крымского ханства?\n",
      "Question (EN): What was the name of the capital of the Crimean khanate?\n",
      "Answer: Бахчисарай\n",
      "\n",
      "Source: {'uid': 526, 'version': '1.0'}\n",
      "Content: Question (RU): Какой город является административным центром Приморского края?\n",
      "Question (EN): Which city is the administrative center of Primorsky Krai?\n",
      "Answer: Владивосток\n",
      "\n",
      "Source: {'uid': 859, 'version': '1.0'}\n",
      "Content: Question (RU): В каком городе находится гробница пророка Мухаммеда?\n",
      "Question (EN): In which city is the tomb of the prophet Muhammad located?\n",
      "Answer: Медина\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Столицей Туркмении является город Ашхабад.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"def234\"}}\n",
    "\n",
    "q1 = \"Какой город является столицей Туркмении?\"\n",
    "q2 = \"Что насчет Удмуртской республики?\"\n",
    "# q1 = \"What is the weather in sf?\"\n",
    "# q2 = \"What about new york?\"\n",
    "\n",
    "input_message = q1\n",
    "\n",
    "for event in agent_executor.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Что насчет Удмуртской республики?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (call_4gav)\n",
      " Call ID: call_4gav\n",
      "  Args:\n",
      "    query: столица Удмуртской республики\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'uid': 221, 'version': '1.0'}\n",
      "Content: Question (RU): Какой город является столицей Удмуртской республики?\n",
      "Question (EN): Which city is the capital of the Udmurt Republic?\n",
      "Answer: Ижевск\n",
      "\n",
      "Source: {'uid': 526, 'version': '1.0'}\n",
      "Content: Question (RU): Какой город является административным центром Приморского края?\n",
      "Question (EN): Which city is the administrative center of Primorsky Krai?\n",
      "Answer: Владивосток\n",
      "\n",
      "Source: {'uid': 6554, 'version': '2.0'}\n",
      "Content: Question (RU): Какой город является столицей республики Карелия?\n",
      "Question (EN): Which city is the capital of the Republic of Karelia?\n",
      "Answer: Петрозаводск\n",
      "\n",
      "Source: {'uid': 6023, 'version': '2.0'}\n",
      "Content: Джебель-Шаммар, Хиджаз, Государство дервишей, Украинская держава, Первая Португальская республика, Ньюфаундленд, Балтийское герцогство, Крымское краевое правительство, Всевеликое войско Донское, Дарфурский султанат, Доминион Новая\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Столицей Удмуртской республики является город Ижевск.\n"
     ]
    }
   ],
   "source": [
    "input_message = q2\n",
    "\n",
    "for event in agent_executor.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "# @tool(\"retrieve_docs\", response_format=\"content\")\n",
    "# def retriever_tool(query: str):\n",
    "#     \"\"\"Retrieve information related to a query.\"\"\"\n",
    "#     # retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "#     retrieved_docs = retriever.invoke(query)\n",
    "#     serialized = \"\\n\\n\".join(\n",
    "#         (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\") for doc in retrieved_docs\n",
    "#     )\n",
    "#     return serialized#, retrieved_docs\n",
    "\n",
    "\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_documents\",\n",
    "    \"Search and return information relevenat to the input question\",\n",
    ")\n",
    "\n",
    "tools = [retriever_tool]\n",
    "\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_core.runnables import RunnableConfig, chain\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    # model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    model = ChatGroq(model=\"llama-3.1-70b-versatile\", temperature=0, streaming=True)\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = [m for m in messages if isinstance(m, HumanMessage)][-1].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\"\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    # model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4-turbo\")\n",
    "    model = ChatGroq(model=\"llama-3.1-70b-versatile\", temperature=0, streaming=True)\n",
    "\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    # question = messages[0].content\n",
    "    question = [m for m in messages if isinstance(m, HumanMessage)][-1].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    # model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "    model = ChatGroq(model=\"llama-3.1-70b-versatile\", temperature=0, streaming=True)\n",
    "\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = [m for m in messages if isinstance(m, HumanMessage)][-1].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    prompt = ChatPromptTemplate(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                (\n",
    "                    \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \"\n",
    "                    \"If you don't know the answer, just say that you don't know. Do not mention that you have used the provided context. \"\n",
    "                    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "                    \"Question: {question}\"\n",
    "                    \"\\nContext: {context} \"\n",
    "                    \"\\nAnswer:\"\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "    )    \n",
    "\n",
    "    # LLM\n",
    "    # llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
    "    llm = ChatGroq(model=\"llama-3.1-70b-versatile\", temperature=0, streaming=True)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [AIMessage(response)]}\n",
    "\n",
    "\n",
    "def web_search(state: AgentState):\n",
    "    \"\"\"\n",
    "    Web search based on the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        dict: Updates documents with appended web results\n",
    "    \"\"\"\n",
    "    messages = state['messages']\n",
    "    question = [m for m in messages if isinstance(m, HumanMessage)][-1].content\n",
    "    # last_message = messages[-1]\n",
    "\n",
    "    # docs = last_message.content\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "\n",
    "    model = ChatGroq(model=\"llama-3.1-70b-versatile\", temperature=0, streaming=True)\n",
    "\n",
    "    web_search_tool = TavilySearchResults()\n",
    "    llm_with_tools = model.bind_tools([web_search_tool])\n",
    "    # response = llm_with_tools.invoke(question)\n",
    "\n",
    "    # docs = web_search_tool.invoke(question)\n",
    "    # web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "\n",
    "    @chain\n",
    "    def tool_chain(user_input: str, config: RunnableConfig):\n",
    "        ai_msg = llm_with_tools.invoke(user_input, config=config)\n",
    "        tool_msgs = web_search_tool.batch(ai_msg.tool_calls, config=config)\n",
    "        return llm_with_tools.invoke(\n",
    "            [ai_msg, *tool_msgs], config=config\n",
    "        )\n",
    "\n",
    "    return {\"messages\": tool_chain.invoke(question)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode([retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "# retrieve = ToolNode([retriever_tool])\n",
    "\n",
    "\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: \"web_search\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"web_search\", END)\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q1 = \"Какой город является столицей Туркмении?\"\n",
    "# q2 = \"Что насчет Удмуртской республики?\"\n",
    "q1 = \"What is the weather in sf?\"\n",
    "q2 = \"What about new york?\"\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the weather in sf?\n",
      "---CALL AGENT---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Tool use failed: JSON does not match the expected schema for tool calls\n",
      "---WEB SEARCH---\n"
     ]
    },
    {
     "ename": "InvalidUpdateError",
     "evalue": "Expected node messages to update at least one of ['messages'], got {'message': AIMessage(content='The current weather in San Francisco is foggy with a temperature of 50.4°F (10.2°C) and a wind speed of 4.7 mph (7.6 kph).', additional_kwargs={}, response_metadata={'finish_reason': 'stop'}, id='run-48a040f9-00c4-4dd9-baeb-b3cce9f46018-0', usage_metadata={'input_tokens': 1390, 'output_tokens': 42, 'total_tokens': 1432})}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[192], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m      2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: q1}]},\n\u001b[1;32m      3\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m      5\u001b[0m ):\n\u001b[1;32m      6\u001b[0m     event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpretty_print()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chatbot-rag-0NodYq9M-py3.10/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1641\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1645\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1646\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1647\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1648\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1649\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1650\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1651\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1652\u001b[0m         ):\n\u001b[1;32m   1653\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chatbot-rag-0NodYq9M-py3.10/lib/python3.10/site-packages/langgraph/pregel/runner.py:104\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    102\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chatbot-rag-0NodYq9M-py3.10/lib/python3.10/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, writer)\u001b[0m\n\u001b[1;32m     38\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chatbot-rag-0NodYq9M-py3.10/lib/python3.10/site-packages/langgraph/utils/runnable.py:412\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chatbot-rag-0NodYq9M-py3.10/lib/python3.10/site-packages/langgraph/utils/runnable.py:176\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m    175\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m--> 176\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    178\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chatbot-rag-0NodYq9M-py3.10/lib/python3.10/site-packages/langgraph/pregel/write.py:85\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     writes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     80\u001b[0m         ChannelWriteEntry(write\u001b[38;5;241m.\u001b[39mchannel, \u001b[38;5;28minput\u001b[39m, write\u001b[38;5;241m.\u001b[39mskip_none, write\u001b[38;5;241m.\u001b[39mmapper)\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m write\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[1;32m     84\u001b[0m     ]\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_at_least_one_of\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chatbot-rag-0NodYq9M-py3.10/lib/python3.10/site-packages/langgraph/pregel/write.py:129\u001b[0m, in \u001b[0;36mChannelWrite.do_write\u001b[0;34m(config, writes, require_at_least_one_of)\u001b[0m\n\u001b[1;32m    127\u001b[0m entries \u001b[38;5;241m=\u001b[39m [write \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m writes \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry)]\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# process entries into values\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    130\u001b[0m     write\u001b[38;5;241m.\u001b[39mmapper(write\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mif\u001b[39;00m write\u001b[38;5;241m.\u001b[39mmapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m entries\n\u001b[1;32m    132\u001b[0m ]\n\u001b[1;32m    133\u001b[0m values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    134\u001b[0m     (write\u001b[38;5;241m.\u001b[39mchannel, val)\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val, write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, entries)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m write\u001b[38;5;241m.\u001b[39mskip_none \u001b[38;5;129;01mor\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m ]\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# filter out SKIP_WRITE values\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chatbot-rag-0NodYq9M-py3.10/lib/python3.10/site-packages/langgraph/pregel/write.py:130\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    127\u001b[0m entries \u001b[38;5;241m=\u001b[39m [write \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m writes \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry)]\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# process entries into values\u001b[39;00m\n\u001b[1;32m    129\u001b[0m values \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 130\u001b[0m     \u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m write\u001b[38;5;241m.\u001b[39mmapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m entries\n\u001b[1;32m    132\u001b[0m ]\n\u001b[1;32m    133\u001b[0m values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    134\u001b[0m     (write\u001b[38;5;241m.\u001b[39mchannel, val)\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val, write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, entries)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m write\u001b[38;5;241m.\u001b[39mskip_none \u001b[38;5;129;01mor\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m ]\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# filter out SKIP_WRITE values\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chatbot-rag-0NodYq9M-py3.10/lib/python3.10/site-packages/langgraph/graph/state.py:635\u001b[0m, in \u001b[0;36mCompiledStateGraph.attach_node.<locals>._get_state_key\u001b[0;34m(input, key)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m output_keys \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m--> 635\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(\n\u001b[1;32m    636\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected node \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to update at least one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    637\u001b[0m         )\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mget(key, SKIP_WRITE)\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, Command):\n",
      "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected node messages to update at least one of ['messages'], got {'message': AIMessage(content='The current weather in San Francisco is foggy with a temperature of 50.4°F (10.2°C) and a wind speed of 4.7 mph (7.6 kph).', additional_kwargs={}, response_metadata={'finish_reason': 'stop'}, id='run-48a040f9-00c4-4dd9-baeb-b3cce9f46018-0', usage_metadata={'input_tokens': 1390, 'output_tokens': 42, 'total_tokens': 1432})}"
     ]
    }
   ],
   "source": [
    "\n",
    "for event in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": q1}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Что насчет Удмуртской республики?\n",
      "---CALL AGENT---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_docs (call_x9nj)\n",
      " Call ID: call_x9nj\n",
      "  Args:\n",
      "    query: столица Удмуртской республики\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_docs\n",
      "\n",
      "Source: {'uid': 221, 'version': '1.0'}\n",
      "Content: Question (RU): Какой город является столицей Удмуртской республики?\n",
      "Question (EN): Which city is the capital of the Udmurt Republic?\n",
      "Answer: Ижевск\n",
      "\n",
      "Source: {'uid': 526, 'version': '1.0'}\n",
      "Content: Question (RU): Какой город является административным центром Приморского края?\n",
      "Question (EN): Which city is the administrative center of Primorsky Krai?\n",
      "Answer: Владивосток\n",
      "\n",
      "Source: {'uid': 6554, 'version': '2.0'}\n",
      "Content: Question (RU): Какой город является столицей республики Карелия?\n",
      "Question (EN): Which city is the capital of the Republic of Karelia?\n",
      "Answer: Петрозаводск\n",
      "\n",
      "Source: {'uid': 6023, 'version': '2.0'}\n",
      "Content: Джебель-Шаммар, Хиджаз, Государство дервишей, Украинская держава, Первая Португальская республика, Ньюфаундленд, Балтийское герцогство, Крымское краевое правительство, Всевеликое войско Донское, Дарфурский султанат, Доминион Новая\n",
      "---GENERATE---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Столицей Удмуртской республики является город Ижевск. Это административный центр республики. Расположен в восточной части Восточно-Европейской равнины.\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": q2}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-rag-0NodYq9M-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
